import streamlit as st
import pandas as pd
from sentence_transformers import SentenceTransformer, util
from PyPDF2 import PdfReader
import tempfile
import os
from datetime import datetime

# åˆå§‹åŒ–æ¨¡å‹
model = SentenceTransformer('all-MiniLM-L6-v2')

st.set_page_config(layout="wide")
st.title("ğŸ“„ è®ºæ–‡åŒ¹é…ä¼šè®®åŠ©æ‰‹")

# åˆå§‹åŒ– session_state
for key in ["conference_file", "paper_file"]:
    if key not in st.session_state:
        st.session_state[key] = None

# å·¦å³å¸ƒå±€
col1, col2 = st.columns(2)

# ä¸Šä¼ ä¼šè®®æ–‡ä»¶
with col1:
    st.subheader("ğŸ“… ä¸Šä¼ ä¼šè®®æ–‡ä»¶")
    conference_uploaded = st.file_uploader("é€‰æ‹©åŒ…å«ä¼šè®®ä¿¡æ¯çš„Excelæ–‡ä»¶", type=["xlsx"], key="conf_uploader")
    if st.button("âŒ æ¸…é™¤ä¼šè®®æ–‡ä»¶"):
        st.session_state.conference_file = None
        conference_uploaded = None

    if conference_uploaded:
        try:
            df = pd.read_excel(conference_uploaded)
            df.columns = df.columns.str.strip()

            # å­—æ®µåæ ‡å‡†åŒ–
            rename_map = {}
            if "ä¼šè®®åç§°" in df.columns:
                rename_map["ä¼šè®®åç§°"] = "ä¼šè®®å"
            df.rename(columns=rename_map, inplace=True)

            required_columns = ["ä¼šè®®å", "ä¼šè®®æ–¹å‘", "ä¼šè®®ä¸»é¢˜æ–¹å‘", "ç»†åˆ†å…³é”®è¯", "ä¼šè®®ç³»åˆ—å", "å®˜ç½‘é“¾æ¥", "åŠ¨æ€å‡ºç‰ˆæ ‡è®°", "æˆªç¨¿æ—¶é—´"]
            missing = [col for col in required_columns if col not in df.columns]
            if missing:
                st.error(f"âŒ ç¼ºå°‘å¿…è¦å­—æ®µï¼š{ ' / '.join(missing) }")
            else:
                st.session_state.conference_file = df
                st.success("âœ… ä¼šè®®æ–‡ä»¶ä¸Šä¼ å¹¶è¯»å–æˆåŠŸ")
        except Exception as e:
            st.error(f"âŒ ä¼šè®®æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{e}")

# ä¸Šä¼ è®ºæ–‡æ–‡ä»¶
with col2:
    st.subheader("ğŸ“ ä¸Šä¼ è®ºæ–‡æ–‡ä»¶ï¼ˆPDFï¼‰")
    paper_uploaded = st.file_uploader("ä¸Šä¼ è®ºæ–‡ PDF æ–‡ä»¶", type=["pdf"], key="paper_uploader")
    if st.button("âŒ æ¸…é™¤è®ºæ–‡æ–‡ä»¶"):
        st.session_state.paper_file = None
        paper_uploaded = None

    if paper_uploaded:
        try:
            # ä¸´æ—¶ä¿å­˜ PDF
            temp_dir = tempfile.mkdtemp()
            temp_path = os.path.join(temp_dir, paper_uploaded.name)
            with open(temp_path, "wb") as f:
                f.write(paper_uploaded.read())

            # è¯»å– PDF æ–‡æœ¬
            reader = PdfReader(temp_path)
            text = ""
            for page in reader.pages:
                text += page.extract_text() + " "
            st.session_state.paper_file = text.strip()
            st.success("âœ… è®ºæ–‡å†…å®¹æå–æˆåŠŸ")
        except Exception as e:
            st.error(f"âŒ PDF å¤„ç†å¤±è´¥ï¼š{e}")

# è®ºæ–‡æ–¹å‘æå–ï¼ˆæ ¸å¿ƒæ”¹è¿›ç‚¹ï¼‰
def extract_paper_research_field(text):
    """
    ä»è®ºæ–‡æ–‡æœ¬ä¸­æå–ç ”ç©¶æ–¹å‘çš„ç®€å•æ–¹æ³•ï¼šå¯ä»¥æ ¹æ®æ–‡æœ¬åµŒå…¥è®¡ç®—ä¸å·²çŸ¥å­¦ç§‘æ–¹å‘çš„ç›¸ä¼¼åº¦
    """
    # å¸¸è§å­¦ç§‘æ–¹å‘ï¼ˆå¯ä»¥æ‹“å±•ï¼‰
    academic_fields = [
        "è®¡ç®—æœºç§‘å­¦", "ç”µå­å·¥ç¨‹", "ç”Ÿç‰©åŒ»å­¦", "åŒ–å­¦", "ç‰©ç†", "ææ–™ç§‘å­¦", "åŒ»å­¦", "äººå·¥æ™ºèƒ½", "æ•°æ®ç§‘å­¦", "ç¤¾ä¼šå­¦",
        "å¿ƒç†å­¦", "ç¯å¢ƒç§‘å­¦", "ç»æµå­¦", "æ•™è‚²å­¦", "ç¤¾ä¼šå­¦", "åœ°ç†å­¦", "æ³•å­¦"
    ]
    
    # ç”¨æ¨¡å‹è®¡ç®—æ–‡æœ¬åµŒå…¥å‘é‡
    paper_embedding = model.encode(text, convert_to_tensor=True)

    # åˆ›å»ºå­¦ç§‘æ–¹å‘çš„åµŒå…¥å‘é‡
    field_embeddings = model.encode(academic_fields, convert_to_tensor=True)

    # è®¡ç®—ä¸æ¯ä¸ªå­¦ç§‘æ–¹å‘çš„ç›¸ä¼¼åº¦
    similarities = util.cos_sim(paper_embedding, field_embeddings).cpu().numpy().flatten()

    # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è¿”å›å‰ä¸‰ä¸ªå­¦ç§‘æ–¹å‘åŠå…¶ç›¸ä¼¼åº¦
    top_indexes = similarities.argsort()[::-1][:3]
    result = [(academic_fields[idx], similarities[idx]) for idx in top_indexes]
    total_similarity = sum([similarity for _, similarity in result])
    
    # è¿”å›å­¦ç§‘æ–¹å‘åŠå…¶å¯¹åº”çš„ç™¾åˆ†æ¯”
    result_with_percentage = [(field, round(similarity / total_similarity * 100, 2)) for field, similarity in result]
    return result_with_percentage

# æ‰§è¡ŒåŒ¹é…
if st.session_state.conference_file is not None and st.session_state.paper_file is not None:
    st.divider()
    st.subheader("ğŸ“Š åŒ¹é…ç»“æœ")

    # æå–è®ºæ–‡çš„å­¦ç§‘æ–¹å‘
    paper_text = st.session_state.paper_file
    paper_fields = extract_paper_research_field(paper_text)

    st.write("### è®ºæ–‡æ¶‰åŠçš„å­¦ç§‘æ–¹å‘ï¼š")
    for field, percentage in paper_fields:
        st.write(f"**{field}**: {percentage}%")

    # å¤„ç†ä¼šè®®æ•°æ®
    results = []
    with st.spinner("æ­£åœ¨è¿›è¡ŒåŒ¹é…ï¼Œè¯·ç¨ç­‰..."):
        st.progress(0)  # åˆå§‹è¿›åº¦æ¡
        total_steps = len(st.session_state.conference_file)
        
        for idx, (_, row) in enumerate(st.session_state.conference_file.iterrows()):
            # è®¡ç®—è¿›åº¦æ¡
            st.progress((idx + 1) / total_steps)

            row_text = " ".join(str(row[col]) for col in ["ä¼šè®®æ–¹å‘", "ä¼šè®®ä¸»é¢˜æ–¹å‘", "ç»†åˆ†å…³é”®è¯"] if pd.notna(row[col]))
            row_embedding = model.encode(row_text, convert_to_tensor=True)
            similarity = util.cos_sim(paper_embedding, row_embedding).item()
            
            # è®¡ç®—æˆªç¨¿æ—¶é—´
            if pd.notna(row.get("æˆªç¨¿æ—¶é—´", None)):
                try:
                    deadline = datetime.strptime(str(row["æˆªç¨¿æ—¶é—´"]), "%Y-%m-%d")
                    days_left = (deadline - datetime.now()).days
                except Exception as e:
                    days_left = "æœªçŸ¥"
            else:
                days_left = "æœªçŸ¥"
            
            # æå–æ¨èä¿¡æ¯
            results.append({
                "ä¼šè®®æ¨èæ ‡é¢˜": f"{row['ä¼šè®®ç³»åˆ—å']} - {row['ä¼šè®®å']}",
                "å®˜ç½‘é“¾æ¥": row["å®˜ç½‘é“¾æ¥"],
                "åŠ¨æ€å‡ºç‰ˆæ ‡è®°": row["åŠ¨æ€å‡ºç‰ˆæ ‡è®°"],
                "è·ç¦»æˆªç¨¿æ—¶é—´(å¤©)": days_left,
                "åŒ¹é…åˆ†æ•°": round(similarity, 4),
                "ä¼šè®®æ–¹å‘": row["ä¼šè®®æ–¹å‘"],
                "è®ºæ–‡ç ”ç©¶æ–¹å‘": paper_fields[0][0],  # åªå±•ç¤ºæœ€é«˜çš„åŒ¹é…å­¦ç§‘
                "ç»†åˆ†å…³é”®è¯": row["ç»†åˆ†å…³é”®è¯"],
                "åŒ¹é…åˆ†æ": f"è¯¥ä¼šè®®çš„ã€{row['ä¼šè®®æ–¹å‘']}ã€‘ä¸è®ºæ–‡çš„ç ”ç©¶æ–¹å‘åŒ¹é…åº¦è¾ƒé«˜ã€‚"  # å¯æ ¹æ®å®é™…éœ€è¦è°ƒæ•´åŒ¹é…æè¿°
            })

    # æ’åºå¹¶ç­›é€‰å‰3ä¸ªæ¨èä¼šè®®
    sorted_results = sorted(results, key=lambda x: x["åŒ¹é…åˆ†æ•°"], reverse=True)
    top_results = sorted_results[:3]

    # æ˜¾ç¤ºç»“æœ
    for result in top_results:
        st.markdown(f"### {result['ä¼šè®®æ¨èæ ‡é¢˜']}")
        st.markdown(f"**å®˜ç½‘é“¾æ¥**: [ç‚¹å‡»è®¿é—®]({result['å®˜ç½‘é“¾æ¥']})")
        st.markdown(f"**åŠ¨æ€å‡ºç‰ˆæ ‡è®°**: {result['åŠ¨æ€å‡ºç‰ˆæ ‡è®°']}")
        st.markdown(f"**è·ç¦»æˆªç¨¿æ—¶é—´**: {result['è·ç¦»æˆªç¨¿æ—¶é—´(å¤©)']} å¤©")
        st.markdown(f"**åŒ¹é…åˆ†æ•°**: {result['åŒ¹é…åˆ†æ•°']}")
        st.markdown(f"**è®ºæ–‡ç ”ç©¶æ–¹å‘ä¸ä¼šè®®åŒ¹é…åˆ†æ**: {result['åŒ¹é…åˆ†æ']}")
        st.markdown(f"**ç»†åˆ†å…³é”®è¯**: {result['ç»†åˆ†å…³é”®è¯']}")
        st.markdown("---")
