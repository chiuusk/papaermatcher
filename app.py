# ç”Ÿæˆå®Œæ•´çš„ app.py æ–‡ä»¶å†…å®¹ï¼Œæ»¡è¶³ç”¨æˆ·æ‰€æœ‰åŠŸèƒ½ä¸UIè¦æ±‚

app_code = '''
import streamlit as st
import pandas as pd
import datetime
import fitz  # PyMuPDF
import docx
import io

# ---------------- æ–‡ä»¶è§£æå‡½æ•° ----------------

def extract_text_from_pdf(file):
    try:
        with fitz.open(stream=file.read(), filetype="pdf") as doc:
            text = ""
            for page in doc:
                text += page.get_text()
        return text
    except Exception as e:
        st.error(f"PDF è§£æå¤±è´¥: {e}")
        return ""

def extract_text_from_docx(file):
    try:
        doc = docx.Document(file)
        text = ""
        for para in doc.paragraphs:
            text += para.text + "\\n"
        return text
    except Exception as e:
        st.error(f"Word è§£æå¤±è´¥: {e}")
        return ""

def extract_paper_text(file):
    if file.name.endswith(".pdf"):
        return extract_text_from_pdf(file)
    elif file.name.endswith(".docx"):
        return extract_text_from_docx(file)
    else:
        return ""

# ---------------- å­¦ç§‘åˆ†æå‡½æ•° ----------------

def analyze_subjects(text):
    subjects_keywords = {
        "ç”µåŠ›ç³»ç»Ÿ": ["power system", "voltage", "rectifier", "ç”µç½‘", "ç”µåŠ›"],
        "æ§åˆ¶ç†è®º": ["control", "PID", "ç¨³å®šæ€§", "æ§åˆ¶å™¨", "æ§åˆ¶ç†è®º"],
        "è®¡ç®—æœºç§‘å­¦": ["algorithm", "data", "ç¥ç»ç½‘ç»œ", "machine learning", "äººå·¥æ™ºèƒ½"],
        "ç”µå­å·¥ç¨‹": ["ä¿¡å·", "ç”µè·¯", "è°ƒåˆ¶", "åµŒå…¥å¼", "sensor"],
        "é€šä¿¡å·¥ç¨‹": ["network", "æ— çº¿", "é€šä¿¡", "5G", "ä¿¡é“"]
    }

    counts = {}
    lower_text = text.lower()
    for subject, keywords in subjects_keywords.items():
        count = sum(lower_text.count(keyword.lower()) for keyword in keywords)
        if count > 0:
            counts[subject] = count

    total = sum(counts.values())
    if total == 0:
        return {}

    percentages = {subject: round(count / total * 100, 2) for subject, count in counts.items()}
    sorted_subjects = dict(sorted(percentages.items(), key=lambda x: x[1], reverse=True))
    return sorted_subjects

# ---------------- åŒ¹é…å‡½æ•° ----------------

def calculate_days_left(cutoff_date):
    if isinstance(cutoff_date, str):
        cutoff_date = pd.to_datetime(cutoff_date).date()
    return (cutoff_date - datetime.datetime.now().date()).days

def match_conferences(conference_data, paper_subjects):
    matches = []
    for _, row in conference_data.iterrows():
        conference_subjects = str(row.get("ä¼šè®®ä¸»é¢˜æ–¹å‘", "")).split(",")
        match_score = 0
        matched = []

        for subject, score in paper_subjects.items():
            for conf_sub in conference_subjects:
                if subject.strip() in conf_sub:
                    match_score += score
                    matched.append(subject)

        if match_score > 0:
            matches.append({
                "ä¼šè®®ç³»åˆ—åä¸ä¼šè®®å": f"{row.get('ä¼šè®®ç³»åˆ—å', '')} - {row.get('ä¼šè®®å', '')}",
                "å®˜ç½‘é“¾æ¥": row.get("å®˜ç½‘é“¾æ¥", ""),
                "åŠ¨æ€å‡ºç‰ˆæ ‡è®°": row.get("åŠ¨æ€å‡ºç‰ˆæ ‡è®°", ""),
                "æˆªç¨¿æ—¶é—´": row.get("æˆªç¨¿æ—¶é—´", ""),
                "å‰©ä½™å¤©æ•°": calculate_days_left(row.get("æˆªç¨¿æ—¶é—´", "")),
                "åŒ¹é…å­¦ç§‘": ", ".join(set(matched))
            })
    return matches

# ---------------- ä¸»åº”ç”¨å‡½æ•° ----------------

def main():
    st.set_page_config(layout="wide")
    st.title("ğŸ“š è®ºæ–‡ä¸ä¼šè®®åŒ¹é…ç³»ç»Ÿ")

    # ä¸¤åˆ—å¸ƒå±€ï¼Œå·¦ä¼šè®®å³è®ºæ–‡
    col1, col2 = st.columns([1, 1])

    with col1:
        st.subheader("ğŸ“ ä¸Šä¼ ä¼šè®®æ–‡ä»¶")
        conference_file = st.file_uploader("ä¸Šä¼  Excel æ ¼å¼çš„ä¼šè®®åˆ—è¡¨", type=["xlsx"], key="conf", label_visibility="collapsed")
        conference_data = None
        if conference_file:
            try:
                conference_data = pd.read_excel(conference_file)
                st.success("ä¼šè®®æ–‡ä»¶åŠ è½½æˆåŠŸ")
            except Exception as e:
                st.error(f"ä¼šè®®æ–‡ä»¶è¯»å–å¤±è´¥: {e}")

    with col2:
        st.subheader("ğŸ“„ ä¸Šä¼ è®ºæ–‡æ–‡ä»¶")
        paper_file = st.file_uploader("ä¸Šä¼ è®ºæ–‡æ–‡ä»¶ (PDF / Word)", type=["pdf", "docx"], key="paper", label_visibility="collapsed")
        if paper_file:
            st.info("å·²ä¸Šä¼ è®ºæ–‡æ–‡ä»¶ï¼Œæ­£åœ¨åˆ†æä¸­...")
            text = extract_paper_text(paper_file)
            if text:
                subjects = analyze_subjects(text)
                if subjects:
                    st.markdown("### ğŸ“Š è®ºæ–‡å­¦ç§‘æ–¹å‘åˆ†æ")
                    for subject, percent in subjects.items():
                        st.write(f"- {subject}: {percent}%")
                else:
                    st.warning("æœªèƒ½è¯†åˆ«æ˜ç¡®çš„å­¦ç§‘æ–¹å‘")
            else:
                st.warning("æ— æ³•è§£æè®ºæ–‡å†…å®¹")

            if conference_file and text and subjects:
                st.markdown("### ğŸ§  æ­£åœ¨åŒ¹é…ä¼šè®®...")
                matches = match_conferences(conference_data, subjects)
                if matches:
                    for match in matches:
                        st.markdown(f"#### ğŸ¯ æ¨èä¼šè®®ï¼š{match['ä¼šè®®ç³»åˆ—åä¸ä¼šè®®å']}")
                        st.markdown(f"- å®˜ç½‘é“¾æ¥: [{match['å®˜ç½‘é“¾æ¥']}]({match['å®˜ç½‘é“¾æ¥']})")
                        st.markdown(f"- åŠ¨æ€å‡ºç‰ˆæ ‡è®°: {match['åŠ¨æ€å‡ºç‰ˆæ ‡è®°']}")
                        st.markdown(f"- æˆªç¨¿æ—¶é—´: {match['æˆªç¨¿æ—¶é—´']} (å‰©ä½™ {match['å‰©ä½™å¤©æ•°']} å¤©)")
                        st.markdown(f"- åŒ¹é…å­¦ç§‘: {match['åŒ¹é…å­¦ç§‘']}")
                else:
                    st.info("æœªæ‰¾åˆ°åŒ¹é…çš„ä¼šè®®ï¼Œè¯·å‚è€ƒå­¦ç§‘æ–¹å‘å¯»æ‰¾æ›´å¤šä¼šè®®ã€‚")

if __name__ == "__main__":
    main()
'''

# ä¿å­˜æˆ app.py æ–‡ä»¶
with open("/mnt/data/app.py", "w", encoding="utf-8") as f:
    f.write(app_code)

"/mnt/data/app.py"
