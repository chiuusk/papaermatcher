import streamlit as st
import pdfplumber
import docx
import pandas as pd
import datetime
import re
import nltk
from sentence_transformers import SentenceTransformer, util

nltk.download('punkt')
from nltk.tokenize import sent_tokenize

model = SentenceTransformer('all-MiniLM-L6-v2')


# ========== å·¥å…·å‡½æ•° ==========

def extract_text_from_pdf(file):
    with pdfplumber.open(file) as pdf:
        text = "\n".join(page.extract_text() for page in pdf.pages if page.extract_text())
    return text


def extract_text_from_docx(file):
    doc = docx.Document(file)
    return "\n".join([para.text for para in doc.paragraphs])


def extract_sections(text):
    text = text.replace('\n', ' ')
    sentences = sent_tokenize(text)

    title = sentences[0] if len(sentences) > 0 else ""
    abstract = next((s for s in sentences if "abstract" in s.lower()), "")
    keywords = re.findall(r"[Kk]eywords?\s*[:ï¼š]?\s*(.*?)\.", text)
    conclusion = next((s for s in sentences[::-1] if "conclusion" in s.lower()), "")

    return {
        "title": title.strip(),
        "abstract": abstract.strip(),
        "keywords": keywords[0].strip() if keywords else "",
        "conclusion": conclusion.strip()
    }


def match_conference(paper_embedding, conference_df):
    results = []

    for _, row in conference_df.iterrows():
        keywords = f"{row['ä¼šè®®æ–¹å‘']} {row['ä¼šè®®ä¸»é¢˜æ–¹å‘']} {row['ç»†åˆ†å…³é”®è¯']}"
        conf_embedding = model.encode(keywords, convert_to_tensor=True)
        score = util.cos_sim(paper_embedding, conf_embedding).item()

        # æå–ä¼šè®®ç›¸å…³å…³é”®è¯å’Œæ–¹å‘
        matched_keywords = []
        for keyword in [row['ä¼šè®®æ–¹å‘'], row['ä¼šè®®ä¸»é¢˜æ–¹å‘'], row['ç»†åˆ†å…³é”®è¯']]:
            if keyword.lower() in paper_embedding.lower():
                matched_keywords.append(keyword)

        results.append({
            "ä¼šè®®åç§°": row["ä¼šè®®åç§°"],
            "å®˜ç½‘é“¾æ¥": row["å®˜ç½‘é“¾æ¥"],
            "æˆªç¨¿æ—¶é—´": row["æˆªç¨¿æ—¶é—´"],
            "åŒ¹é…åº¦": score,
            "åŒ¹é…å…³é”®è¯": matched_keywords,
            "ä¼šè®®æ–¹å‘": row['ä¼šè®®æ–¹å‘'],
            "ç»†åˆ†å…³é”®è¯": row['ç»†åˆ†å…³é”®è¯']
        })

    top2 = sorted(results, key=lambda x: x["åŒ¹é…åº¦"], reverse=True)[:2]

    # è®¡ç®—è·ç¦»æˆªç¨¿æ—¶é—´
    for conf in top2:
        try:
            conf["å‰©ä½™å¤©æ•°"] = (
                datetime.datetime.strptime(conf["æˆªç¨¿æ—¶é—´"], "%Y-%m-%d") - datetime.datetime.now()
            ).days
        except:
            conf["å‰©ä½™å¤©æ•°"] = "æœªçŸ¥"
    
    return top2


# ========== Streamlit ç•Œé¢ ==========

st.title("ğŸ“„ è®ºæ–‡å­¦ç§‘æ–¹å‘è¯†åˆ«ä¸ä¼šè®®æ¨èå·¥å…·")

# ä¸Šä¼ è®ºæ–‡
paper_file = st.file_uploader("ä¸Šä¼ è®ºæ–‡ï¼ˆPDF æˆ– Wordï¼‰", type=["pdf", "docx"])
if paper_file:
    if paper_file.type == "application/pdf":
        full_text = extract_text_from_pdf(paper_file)
    else:
        full_text = extract_text_from_docx(paper_file)

    sections = extract_sections(full_text)
    st.subheader("ğŸ“Œ æå–ä¿¡æ¯")
    st.markdown(f"**æ ‡é¢˜**: {sections['title']}")
    st.markdown(f"**æ‘˜è¦**: {sections['abstract']}")
    st.markdown(f"**å…³é”®è¯**: {sections['keywords']}")
    st.markdown(f"**ç»“è®ºæ®µè½**: {sections['conclusion']}")

    # è®ºæ–‡å‘é‡åŒ–
    combined_text = " ".join([
        sections["title"], sections["abstract"], sections["keywords"], sections["conclusion"]
    ])
    paper_embedding = model.encode(combined_text, convert_to_tensor=True)

    # ä¸Šä¼ ä¼šè®®æ–‡ä»¶
    st.subheader("ğŸ“‹ ä¸Šä¼ ä¼šè®®ä¿¡æ¯æ–‡ä»¶ï¼ˆCSVï¼‰")
    conference_file = st.file_uploader("åŒ…å«å­—æ®µï¼šä¼šè®®åç§°ã€ä¼šè®®æ–¹å‘ã€ä¼šè®®ä¸»é¢˜æ–¹å‘ã€ç»†åˆ†å…³é”®è¯ã€æˆªç¨¿æ—¶é—´ï¼ˆYYYY-MM-DDï¼‰ã€å®˜ç½‘é“¾æ¥", type=["csv"])

    if conference_file:
        conf_df = pd.read_csv(conference_file)
        st.success("ä¼šè®®ä¿¡æ¯è¯»å–æˆåŠŸï¼Œå…±åŠ è½½ {} æ¡è®°å½•ã€‚".format(len(conf_df)))

        recommendations = match_conference(paper_embedding, conf_df)

        st.subheader("ğŸ¯ æ¨èä¼šè®®")
        for rec in recommendations:
            st.markdown(f"""
            ### [{rec['ä¼šè®®åç§°']}]({rec['å®˜ç½‘é“¾æ¥']})
            - **åŒ¹é…ç†ç”±**: è®ºæ–‡ä¸­çš„å…³é”®è¯ä¸ä¼šè®®æ–¹å‘åŒ¹é…ï¼š{', '.join(rec['åŒ¹é…å…³é”®è¯'])}
            - **åŒ¹é…åº¦**: {rec['åŒ¹é…åº¦']:.2f}
            - **è·ç¦»æˆªç¨¿æ—¶é—´**: {rec['å‰©ä½™å¤©æ•°']} å¤©
            """)

# åº•éƒ¨
st.markdown("---")
st.markdown("ç”± GPT + Sentence Transformers æä¾›è¯­ä¹‰åˆ†ææ”¯æŒ")
