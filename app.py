import streamlit as st
import pdfplumber
import docx
import pandas as pd
import datetime
import re
from sentence_transformers import SentenceTransformer, util

# ä½¿ç”¨è½»é‡æ¨¡å‹å¹¶å¼ºåˆ¶ CPU
model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')

# ========== å·¥å…·å‡½æ•° ==========

def extract_text_from_pdf(file):
    with pdfplumber.open(file) as pdf:
        text = "\n".join(page.extract_text() for page in pdf.pages if page.extract_text())
    return text

def extract_text_from_docx(file):
    doc = docx.Document(file)
    return "\n".join([para.text for para in doc.paragraphs])

def split_into_sentences(text):
    # ç®€æ˜“å¥å­åˆ‡åˆ†ï¼Œä¸ä¾èµ– nltk
    text = text.replace('\n', ' ')
    return re.split(r'(?<=[ã€‚.!?])\s+', text)

def extract_sections(text):
    sentences = split_into_sentences(text)
    title = sentences[0] if len(sentences) > 0 else ""
    abstract = ""
    conclusion = ""

    for i, sentence in enumerate(sentences):
        if 'abstract' in sentence.lower():
            abstract = ' '.join(sentences[i:i+3])
        elif 'å…³é”®è¯' in sentence or 'keywords' in sentence.lower():
            keywords_match = re.search(r'(å…³é”®è¯|keywords)\s*[:ï¼š]?\s*(.*)', sentence, re.IGNORECASE)
            if keywords_match:
                keywords = keywords_match.group(2).strip()
            else:
                keywords = ""
        elif 'conclusion' in sentence.lower() or 'ç»“è®º' in sentence:
            conclusion = ' '.join(sentences[i:i+3])

    return {
        "title": title.strip(),
        "abstract": abstract.strip(),
        "keywords": keywords if 'keywords' in locals() else "",
        "conclusion": conclusion.strip()
    }

def match_conference(paper_embedding, conference_df):
    results = []

    for _, row in conference_df.iterrows():
        conf_text = f"{row['ä¼šè®®æ–¹å‘']} {row['ä¼šè®®ä¸»é¢˜æ–¹å‘']} {row['ç»†åˆ†å…³é”®è¯']}"
        conf_embedding = model.encode(conf_text, convert_to_tensor=True)
        score = util.cos_sim(paper_embedding, conf_embedding).item()

        matched_terms = []
        for term in row['ç»†åˆ†å…³é”®è¯'].split(','):
            if term.strip().lower() in conf_text.lower():
                matched_terms.append(term.strip())

        results.append({
            "ä¼šè®®åç§°": row["ä¼šè®®åç§°"],
            "å®˜ç½‘é“¾æ¥": row["å®˜ç½‘é“¾æ¥"],
            "æˆªç¨¿æ—¶é—´": row["æˆªç¨¿æ—¶é—´"],
            "åŒ¹é…åº¦": score,
            "åŒ¹é…å…³é”®è¯": matched_terms,
            "ä¼šè®®æ–¹å‘": row['ä¼šè®®æ–¹å‘'],
            "ç»†åˆ†å…³é”®è¯": row['ç»†åˆ†å…³é”®è¯']
        })

    top2 = sorted(results, key=lambda x: x["åŒ¹é…åº¦"], reverse=True)[:2]

    for conf in top2:
        try:
            conf["å‰©ä½™å¤©æ•°"] = (
                datetime.datetime.strptime(conf["æˆªç¨¿æ—¶é—´"], "%Y-%m-%d") - datetime.datetime.now()
            ).days
        except:
            conf["å‰©ä½™å¤©æ•°"] = "æœªçŸ¥"

    return top2

# ========== Streamlit UI ==========

st.title("ğŸ“„ è®ºæ–‡å­¦ç§‘æ–¹å‘è¯†åˆ«ä¸ä¼šè®®æ¨èå·¥å…·")

paper_file = st.file_uploader("ä¸Šä¼ è®ºæ–‡ï¼ˆPDF æˆ– Wordï¼‰", type=["pdf", "docx"])
if paper_file:
    if paper_file.type == "application/pdf":
        full_text = extract_text_from_pdf(paper_file)
    else:
        full_text = extract_text_from_docx(paper_file)

    sections = extract_sections(full_text)
    st.subheader("ğŸ“Œ æå–ä¿¡æ¯")
    st.markdown(f"**æ ‡é¢˜**: {sections['title']}")
    st.markdown(f"**æ‘˜è¦**: {sections['abstract']}")
    st.markdown(f"**å…³é”®è¯**: {sections['keywords']}")
    st.markdown(f"**ç»“è®ºæ®µè½**: {sections['conclusion']}")

    combined_text = " ".join([
        sections["title"], sections["abstract"], sections["keywords"], sections["conclusion"]
    ])
    paper_embedding = model.encode(combined_text, convert_to_tensor=True)

    st.subheader("ğŸ“‹ ä¸Šä¼ ä¼šè®®ä¿¡æ¯æ–‡ä»¶ï¼ˆCSVï¼‰")
    conference_file = st.file_uploader("åŒ…å«å­—æ®µï¼šä¼šè®®åç§°ã€ä¼šè®®æ–¹å‘ã€ä¼šè®®ä¸»é¢˜æ–¹å‘ã€ç»†åˆ†
