import streamlit as st
import pandas as pd
import datetime
import re
from PyPDF2 import PdfReader
import docx
from sentence_transformers import SentenceTransformer, util

st.set_page_config(layout="wide")
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

st.title("ğŸ“š è®ºæ–‡åŒ¹é…ä¼šè®®åŠ©æ‰‹")

# ä¸Šä¼ ä¼šè®®æ–‡ä»¶ï¼ˆå›ºå®šä¸€æ¬¡ï¼‰
st.sidebar.header("ä¼šè®®æ–‡ä»¶")
conference_file = st.sidebar.file_uploader("ä¸Šä¼ ä¼šè®®æ–‡ä»¶ï¼ˆåªéœ€ä¸€æ¬¡ï¼‰", type=["xlsx"], key="conf")

if conference_file:
    try:
        df_conf = pd.read_excel(conference_file)
        df_conf.columns = df_conf.columns.str.strip()
        df_conf.rename(columns={
            "ä¼šè®®åç§°": "ä¼šè®®å",
            "ä¼šè®®ç³»åˆ—å": "ä¼šè®®ç³»åˆ—å",
            "ä¼šè®®ä¸»é¢˜æ–¹å‘": "ä¼šè®®ä¸»é¢˜æ–¹å‘",
            "ç»†åˆ†æ–¹å‘": "ç»†åˆ†å…³é”®è¯",
            "æ˜¯å¦åŠ¨æ€å‡ºç‰ˆ": "åŠ¨æ€å‡ºç‰ˆæ ‡è®°",
            "æˆªç¨¿æ—¥æœŸ": "æˆªç¨¿æ—¶é—´"
        }, inplace=True)
        st.session_state.conference_df = df_conf
        st.success("âœ… ä¼šè®®æ–‡ä»¶å·²ä¸Šä¼ æˆåŠŸ")
    except Exception as e:
        st.error(f"âŒ ä¼šè®®æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{e}")

# ä¸Šä¼ è®ºæ–‡æ–‡ä»¶
st.header("ä¸Šä¼ è®ºæ–‡æ–‡ä»¶")
paper_file = st.file_uploader("ä¸Šä¼ è®ºæ–‡æ–‡ä»¶ï¼ˆPDF æˆ– Wordï¼‰", type=["pdf", "docx"], key="paper")

# æ–‡æœ¬æå–
def extract_text_from_pdf(file):
    reader = PdfReader(file)
    return "\n".join([p.extract_text() for p in reader.pages[:3] if p.extract_text()])

def extract_text_from_docx(file):
    doc = docx.Document(file)
    return "\n".join([p.text for p in doc.paragraphs])

# æå–è®ºæ–‡ä¿¡æ¯
def extract_paper_info(text):
    lines = text.strip().split("\n")
    title = lines[0] if lines else "æœªçŸ¥æ ‡é¢˜"
    abstract_match = re.search(r"(Abstract|æ‘˜è¦)[\s:ï¼š]*(.+?)(\n|Keywords|å…³é”®è¯)", text, re.DOTALL | re.IGNORECASE)
    keywords_match = re.search(r"(Keywords|å…³é”®è¯)[\s:ï¼š]*(.+)", text, re.IGNORECASE)
    abstract = abstract_match.group(2).strip() if abstract_match else ""
    keywords = keywords_match.group(2).strip() if keywords_match else ""
    return title, abstract, keywords

# å­¦ç§‘æ–¹å‘æ ‡ç­¾
directions = {
    "ç”µåŠ›ç”µå­": ["PWM", "inverter", "rectifier", "ç”µæºæ§åˆ¶"],
    "æ§åˆ¶å·¥ç¨‹": ["PI control", "é—­ç¯", "æ§åˆ¶ç³»ç»Ÿ", "reinforcement learning"],
    "äººå·¥æ™ºèƒ½": ["æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "æœºå™¨å­¦ä¹ "],
    "é€šä¿¡æŠ€æœ¯": ["ä¿¡é“", "è°ƒåˆ¶", "é€šä¿¡åè®®"],
    "ææ–™ç§‘å­¦": ["ææ–™æ€§èƒ½", "å¾®è§‚ç»“æ„", "åˆæˆ"],
    "å¿ƒç†å­¦": ["è®¤çŸ¥", "è¡Œä¸º", "å¿ƒç†æµ‹é‡"],
    "ç¤¾ä¼šå­¦": ["äººå£", "ç¤¾ä¼šè¡Œä¸º", "åŸå¸‚åŒ–"],
    "åŒ»å­¦": ["ç–¾ç—…", "æ²»ç–—", "ç—…ä¾‹"]
}

if paper_file and "conference_df" in st.session_state:
    with st.spinner("æ­£åœ¨åˆ†æè®ºæ–‡..."):
        # æå–æ–‡æœ¬
        if paper_file.type == "application/pdf":
            text = extract_text_from_pdf(paper_file)
        else:
            text = extract_text_from_docx(paper_file)

        title, abstract, keywords = extract_paper_info(text)
        full_text = f"{title} {abstract} {keywords}"
        embedding = model.encode(full_text, convert_to_tensor=True)

        # å­¦ç§‘æ–¹å‘åˆ†æ
        st.subheader("ğŸ” å­¦ç§‘æ–¹å‘è¯†åˆ«")
        direction_names = list(directions.keys())
        dir_embeddings = model.encode(direction_names, convert_to_tensor=True)
        sims = util.cos_sim(embedding, dir_embeddings)[0]
        top_indices = sims.argsort(descending=True)[:3]
        for idx in top_indices:
            dname = direction_names[idx]
            reason = ", ".join([kw for kw in directions[dname] if kw.lower() in full_text.lower()])
            reason = reason if reason else "å…³é”®è¯åŒ¹é…åº¦é«˜"
            st.markdown(f"- **{dname}**ï¼šç›¸å…³è¯ - {reason}")

        # åŒ¹é…ä¼šè®®
        st.subheader("ğŸ¯ åŒ¹é…ç»“æœï¼ˆå« Symposium çš„ä¼šè®®ï¼‰")
        results = []
        for _, row in st.session_state.conference_df.iterrows():
            if "Symposium" not in str(row["ä¼šè®®å"]):
                continue
            conf_text = f"{row['ä¼šè®®å']} {row.get('ä¼šè®®ä¸»é¢˜æ–¹å‘','')} {row.get('ç»†åˆ†å…³é”®è¯','')}"
            conf_embedding = model.encode(conf_text, convert_to_tensor=True)
            score = util.cos_sim(embedding, conf_embedding).item()
            results.append({
                "åŒ¹é…åº¦": score,
                "æ¨èä¼šè®®": f"{row['ä¼šè®®ç³»åˆ—å']} - {row['ä¼šè®®å']}",
                "ä¼šè®®ä¸»é¢˜æ–¹å‘": row.get("ä¼šè®®ä¸»é¢˜æ–¹å‘", ""),
                "ç»†åˆ†å…³é”®è¯": row.get("ç»†åˆ†å…³é”®è¯", ""),
                "åŠ¨æ€å‡ºç‰ˆæ ‡è®°": row.get("åŠ¨æ€å‡ºç‰ˆæ ‡è®°", ""),
                "å®˜ç½‘é“¾æ¥": row.get("å®˜ç½‘é“¾æ¥", ""),
                "è·ç¦»æˆªç¨¿è¿˜æœ‰": (row["æˆªç¨¿æ—¶é—´"] - datetime.datetime.now().date()).days if pd.notna(row.get("æˆªç¨¿æ—¶é—´")) else "æœªçŸ¥",
                "åŒ¹é…ç†ç”±": f"è®ºæ–‡ä¸å…³é”®è¯ã€{row.get('ç»†åˆ†å…³é”®è¯', '')}ã€‘å’Œæ–¹å‘ã€{row.get('ä¼šè®®ä¸»é¢˜æ–¹å‘', '')}ã€‘ç›¸ç¬¦"
            })

        top_matches = sorted(results, key=lambda x: x["åŒ¹é…åº¦"], reverse=True)[:3]
        for match in top_matches:
            st.markdown(f"### ğŸ“Œ {match['æ¨èä¼šè®®']}")
            st.markdown(f"- **ä¼šè®®ä¸»é¢˜æ–¹å‘**ï¼š{match['ä¼šè®®ä¸»é¢˜æ–¹å‘']}")
            st.markdown(f"- **ç»†åˆ†å…³é”®è¯**ï¼š{match['ç»†åˆ†å…³é”®è¯']}")
            st.markdown(f"- **åŠ¨æ€å‡ºç‰ˆæ ‡è®°**ï¼š{match['åŠ¨æ€å‡ºç‰ˆæ ‡è®°']}")
            st.markdown(f"- **å®˜ç½‘é“¾æ¥**ï¼š[ç‚¹æ­¤æŸ¥çœ‹]({match['å®˜ç½‘é“¾æ¥']})")
            st.markdown(f"- **è·ç¦»æˆªç¨¿è¿˜æœ‰**ï¼š{match['è·ç¦»æˆªç¨¿è¿˜æœ‰']} å¤©")
            st.markdown(f"- **åŒ¹é…ç†ç”±**ï¼š{match['åŒ¹é…ç†ç”±']}")
            st.markdown("---")
