import streamlit as st
import pandas as pd
import datetime
from PyPDF2 import PdfReader
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import requests
from bs4 import BeautifulSoup

st.set_page_config(page_title="è®ºæ–‡åŒ¹é…æ¨è", layout="wide")
st.title("ğŸ“š è®ºæ–‡ä¼šè®®åŒ¹é…æ¨èç³»ç»Ÿ")

st.sidebar.header("1ï¸âƒ£ ä¸Šä¼ è®ºæ–‡ PDF")
pdf_file = st.sidebar.file_uploader("ä¸Šä¼ è®ºæ–‡ PDF æ–‡ä»¶", type=["pdf"])

st.sidebar.header("2ï¸âƒ£ ä¸Šä¼ ä¼šè®®åˆ—è¡¨ Excel")
conf_file = st.sidebar.file_uploader("ä¸Šä¼ ä¼šè®® Excel æ–‡ä»¶", type=["xlsx"])

st.sidebar.header("3ï¸âƒ£ è®¾ç½®åŒ¹é…å‚æ•°")
days_today = datetime.datetime.now()

if pdf_file and conf_file:
    # è¯»å– PDF æ–‡ä»¶
    pdf_reader = PdfReader(pdf_file)
    text = ""
    for page in pdf_reader.pages[:2]:  # å–å‰ä¸¤é¡µæå–ä¿¡æ¯
        text += page.extract_text() or ""

    # ç®€å•æå–å…³é”®è¯ï¼ˆå¯æ¢æ›´å¤æ‚çš„æ¨¡å‹ï¼‰
    vectorizer = TfidfVectorizer(stop_words='english', max_features=10)
    tfidf = vectorizer.fit_transform([text])
    paper_keywords = vectorizer.get_feature_names_out()

    st.markdown("### ğŸ“„ è®ºæ–‡æå–ä¿¡æ¯")
    st.write("**è‡ªåŠ¨è¯†åˆ«å…³é”®è¯ï¼š**", ", ".join(paper_keywords))

    # è¯»å–ä¼šè®®æ•°æ®
    df = pd.read_excel(conf_file)
    st.write("ä¼šè®®åˆ—è¡¨ï¼š", df.head())  # æ˜¾ç¤ºä¼šè®®æ•°æ®ï¼Œå¸®åŠ©æ£€æŸ¥æ˜¯å¦è¯»å–æˆåŠŸ

    # å‡è®¾ä¼šè®®åˆ—è¡¨ä¸­æœ‰"ä¼šè®®æ ‡é¢˜"åˆ—å’Œ"ä¼šè®®ä¸»é¢˜"åˆ—ï¼Œä½ å¯ä»¥ä¿®æ”¹ä¸ºå®é™…åˆ—å
    conference_titles = df['Conference Title']  # æ›¿æ¢ä¸ºå®é™…çš„ä¼šè®®æ ‡é¢˜åˆ—
    conference_topics = df['Conference Topics']  # æ›¿æ¢ä¸ºå®é™…çš„ä¼šè®®ä¸»é¢˜åˆ—
    
    # æŠ“å–ä¼šè®®ç½‘ç«™çš„å…³é”®è¯ï¼ˆå‡è®¾ä¼šè®®ç½‘ç«™åˆ—åœ¨ Excel ä¸­ï¼‰
    def get_conference_keywords(url):
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        keywords_meta = soup.find('meta', {'name': 'keywords'})
        if keywords_meta:
            return keywords_meta.get('content', '')
        else:
            keywords_div = soup.find('div', {'class': 'conference-keywords'})
            return keywords_div.text.strip() if keywords_div else "æ— å…³é”®è¯"

    # è·å–ä¼šè®®å…³é”®è¯ï¼ˆå‡è®¾ä¼šè®® Excel æ–‡ä»¶ä¸­æœ‰ URL åˆ—ï¼‰
    conference_keywords = []
    for url in df['Conference URL']:  # å‡è®¾ä¼šè®® URL åˆ—åä¸º 'Conference URL'
        keywords = get_conference_keywords(url)
        conference_keywords.append(keywords)

    df['Keywords'] = conference_keywords

    # åˆå¹¶ä¼šè®®æ ‡é¢˜ã€ä¸»é¢˜å’Œçˆ¬å–çš„å…³é”®è¯ï¼Œä½œä¸ºä¸è®ºæ–‡åŒ¹é…çš„æ–‡æœ¬æ•°æ®
    conference_texts = conference_titles + " " + conference_topics + " " + df['Keywords']
    
    # è®¡ç®—è®ºæ–‡ä¸æ¯ä¸ªä¼šè®®çš„ç›¸ä¼¼åº¦ï¼ˆåŸºäºä¼šè®®æ ‡é¢˜ã€ä¸»é¢˜å’Œå…³é”®è¯ï¼‰
    vectorizer = TfidfVectorizer(stop_words='english')
    conf_tfidf = vectorizer.fit_transform(conference_texts)
    paper_tfidf = vectorizer.transform([text])

    # è®¡ç®—ç›¸ä¼¼åº¦
    similarities = cosine_similarity(paper_tfidf, conf_tfidf)
    
    # æ‰¾åˆ°æœ€åŒ¹é…çš„ä¼šè®®
    matched_conferences = df.iloc[similarities.argmax()]
    st.write("### æœ€åŒ¹é…çš„ä¼šè®®")
    st.write(matched_conferences)
    
    # åœ¨ä¸Šä¼ æ–‡ä»¶ååˆ·æ–°é¡µé¢
    st.experimental_rerun()
