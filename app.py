import streamlit as st
import pandas as pd
import datetime
import requests
from bs4 import BeautifulSoup
from PyPDF2 import PdfReader
from docx import Document
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# é¡µé¢é…ç½®
st.set_page_config(page_title="è®ºæ–‡ä¼šè®®æ™ºèƒ½æ¨è", layout="wide")
st.title("ğŸ“š è®ºæ–‡ä¼šè®®æ™ºèƒ½æ¨èç³»ç»Ÿ")

# æ–‡ä»¶ä¸Šä¼ éƒ¨åˆ†
st.sidebar.header("1ï¸âƒ£ ä¸Šä¼ è®ºæ–‡æ–‡ä»¶ï¼ˆPDF æˆ– Wordï¼‰")
paper_file = st.sidebar.file_uploader("ä¸Šä¼  PDF æˆ– Word æ–‡ä»¶", type=["pdf", "docx"])

st.sidebar.header("2ï¸âƒ£ ä¸Šä¼ ä¼šè®® Excel æ–‡ä»¶")
conf_file = st.sidebar.file_uploader("ä¸Šä¼ ä¼šè®®åˆ—è¡¨æ–‡ä»¶ï¼ˆExcelï¼‰", type=["xlsx"])

# è·å–å½“å‰æ—¶é—´
now = datetime.datetime.now()

# æå–è®ºæ–‡æ–‡æœ¬
def extract_text_from_file(uploaded_file):
    text = ""
    if uploaded_file.name.endswith(".pdf"):
        reader = PdfReader(uploaded_file)
        for page in reader.pages[:2]:
            text += page.extract_text() or ""
    elif uploaded_file.name.endswith(".docx"):
        doc = Document(uploaded_file)
        for para in doc.paragraphs:
            text += para.text + "\n"
    return text

# ä»ä¼šè®®å®˜ç½‘æå–å…³é”®è¯
def extract_keywords_from_url(url):
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        text = soup.get_text()
        return text
    except:
        return ""

# ä¸»é€»è¾‘
if paper_file and conf_file:
    with st.spinner("æ­£åœ¨åˆ†æè®ºæ–‡ä¸ä¼šè®®åŒ¹é…åº¦..."):
        try:
            # 1. è¯»å–è®ºæ–‡å†…å®¹
            paper_text = extract_text_from_file(paper_file)

            # æå–å…³é”®è¯
            tfidf_vec = TfidfVectorizer(stop_words='english', max_features=10)
            paper_tfidf = tfidf_vec.fit_transform([paper_text])
            paper_keywords = tfidf_vec.get_feature_names_out()

            st.markdown("### ğŸ“„ è‡ªåŠ¨æå–çš„è®ºæ–‡å…³é”®è¯")
            st.write(", ".join(paper_keywords))

            # 2. è¯»å–ä¼šè®®æ•°æ®
            df = pd.read_excel(conf_file)

            # åˆ—åæ˜ å°„
            df = df.rename(columns={"ä¼šè®®å": "ä¼šè®®åç§°"})

            required_columns = ["ä¼šè®®ç³»åˆ—å", "ä¼šè®®åç§°", "å½“å‰çŠ¶æ€", "å®˜ç½‘é“¾æ¥", "ä¼šè®®åœ°ç‚¹", "ä¼šè®®æ–¹å‘", "ä¼šè®®ä¸»é¢˜æ–¹å‘", "ç»†åˆ†å…³é”®è¯", "æˆªç¨¿æ—¶é—´"]
            if not all(col in df.columns for col in required_columns):
                st.error(f"Excel ç¼ºå°‘å¿…è¦å­—æ®µï¼Œè¯·ç¡®ä¿åŒ…å«ï¼š{', '.join(required_columns)}")
            else:
                # 3. è¿‡æ»¤æ¡ä»¶
                df = df[
                    (df["å½“å‰çŠ¶æ€"] == "å¾ç¨¿é˜¶æ®µ") &
                    (df["å®˜ç½‘é“¾æ¥"].notna()) &
                    (df["ä¼šè®®åœ°ç‚¹"].notna())
                ]

                # æ„å»ºä¼šè®®å…³é”®è¯æ–‡æœ¬
                df["ç»¼åˆå…³é”®è¯"] = df[["ä¼šè®®æ–¹å‘", "ä¼šè®®ä¸»é¢˜æ–¹å‘", "ç»†åˆ†å…³é”®è¯"]].astype(str).agg(" ".join, axis=1)

                # è®¿é—®å®˜ç½‘å†…å®¹å¹¶é™„åŠ 
                website_texts = []
                for link in df["å®˜ç½‘é“¾æ¥"]:
                    website_texts.append(extract_keywords_from_url(link))
                df["å®˜ç½‘å†…å®¹"] = website_texts

                # åˆå¹¶å…³é”®è¯å’Œç½‘é¡µæ–‡æœ¬åšåŒ¹é…
                df["åŒ¹é…æ–‡æœ¬"] = df["ç»¼åˆå…³é”®è¯"] + " " + df["å®˜ç½‘å†…å®¹"]

                # è®¡ç®—ç›¸ä¼¼åº¦
                conf_tfidf = tfidf_vec.transform(df["åŒ¹é…æ–‡æœ¬"])
                similarity = cosine_similarity(paper_tfidf, conf_tfidf).flatten()
                df["åŒ¹é…åº¦"] = similarity

                # è·ç¦»æˆªç¨¿æ—¶é—´
                df["è·ç¦»æˆªç¨¿"] = df["æˆªç¨¿æ—¶é—´"].apply(lambda d: (pd.to_datetime(d) - now).days if pd.notnull(d) else None)

                # æ¨èå‰2å
                top_matches = df.sort_values(by="åŒ¹é…åº¦", ascending=False).head(2)

                st.markdown("### ğŸ† æ¨èä¼šè®®")
                for _, row in top_matches.iterrows():
                    st.markdown(f"""
                    #### {row['ä¼šè®®ç³»åˆ—å']} - {row['ä¼šè®®åç§°']}
                    - **å®˜ç½‘é“¾æ¥ï¼š** [{row['å®˜ç½‘é“¾æ¥']}]({row['å®˜ç½‘é“¾æ¥']})
                    - **åŒ¹é…ç†ç”±ï¼š** å…³é”®è¯å†…å®¹ç›¸ç¬¦ï¼ˆåŒ¹é…åº¦: {row['åŒ¹é…åº¦']:.2f}ï¼‰
                    - **è·ç¦»æˆªç¨¿æ—¶é—´ï¼š** {row['è·ç¦»æˆªç¨¿']} å¤©
                    """)

                st.success("æ¨èå®Œæˆï¼")

        except Exception as e:
            st.error(f"è¿è¡Œå‡ºé”™ï¼š{e}")
