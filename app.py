import streamlit as st
import pandas as pd
import datetime
import time
import re
import fitz  # pymupdfï¼Œç”¨äºè§£æ PDF
import docx  # ç”¨äºè§£æ Word æ–‡ä»¶

# ------------------- åŸºç¡€å‡½æ•° -------------------

def calculate_days_left(cutoff_date):
    return (cutoff_date - datetime.datetime.now().date()).days

def extract_text_from_pdf(pdf_file):
    doc = fitz.open(stream=pdf_file.read(), filetype="pdf")
    text = ""
    for page in doc:
        text += page.get_text()
    return text

def extract_text_from_docx(docx_file):
    doc = docx.Document(docx_file)
    return "\n".join([para.text for para in doc.paragraphs])

# æå–è®ºæ–‡é¢˜ç›®
def extract_title(text):
    lines = text.strip().split('\n')
    for line in lines:
        if len(line.strip()) > 5 and len(line.strip()) < 200:
            return line.strip()
    return "æœªè¯†åˆ«åˆ°æ ‡é¢˜"

# æå–å…³é”®è¯
def extract_keywords(text):
    patterns = [
        r"(?i)(å…³é”®è¯|Key words|Keywords)[:ï¼š]?\s*(.+)",
    ]
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(2).strip()
    return "æœªè¯†åˆ«åˆ°å…³é”®è¯"

# ç®€å•ä¸­è‹±æ–‡åˆ¤æ–­ï¼ˆç²—ç•¥ï¼‰
def is_chinese(text):
    return any('\u4e00' <= ch <= '\u9fff' for ch in text)

# æ¨¡æ‹Ÿå­¦ç§‘åˆ†æ
def analyze_paper_subject(text):
    # è¿™é‡Œå¯æ›¿æ¢ä¸ºæ›´å¤æ‚çš„ NLP æ¨¡å‹åˆ†æ
    subjects = {
        "ç”µåŠ›ç³»ç»Ÿ": 40,
        "æ§åˆ¶ç†è®º": 35,
        "è®¡ç®—æœºç§‘å­¦": 25
    }
    return subjects

# ------------------- æ–‡ä»¶ä¸Šä¼  -------------------

def upload_conference_file():
    return st.file_uploader("ä¸Šä¼ ä¼šè®®æ–‡ä»¶", type=["xlsx"], key="conference_file")

def upload_paper_file():
    return st.file_uploader("ä¸Šä¼ è®ºæ–‡æ–‡ä»¶", type=["pdf", "docx"], key="paper_file")

# ------------------- ä¸»åŒ¹é…é€»è¾‘ -------------------

def perform_matching(conference_file, paper_subjects):
    if conference_file is None:
        st.warning("æœªä¸Šä¼ ä¼šè®®æ–‡ä»¶ï¼Œæ— æ³•è¿›è¡Œä¼šè®®åŒ¹é…")
        return

    try:
        df = pd.read_excel(conference_file)
        st.success("ä¼šè®®æ–‡ä»¶åŠ è½½æˆåŠŸ")

        matching_conferences = []
        for _, row in df.iterrows():
            conference_subjects = row['ä¼šè®®ä¸»é¢˜æ–¹å‘'].split(',') if pd.notna(row['ä¼šè®®ä¸»é¢˜æ–¹å‘']) else []
            matching_score = sum(paper_subjects.get(subject.strip(), 0) for subject in conference_subjects)

            if matching_score > 0:
                matching_conferences.append({
                    "ä¼šè®®ç³»åˆ—åä¸ä¼šè®®å": f"{row['ä¼šè®®ç³»åˆ—å']} - {row['ä¼šè®®å']}",
                    "å®˜ç½‘é“¾æ¥": row['å®˜ç½‘é“¾æ¥'],
                    "åŠ¨æ€å‡ºç‰ˆæ ‡è®°": row['åŠ¨æ€å‡ºç‰ˆæ ‡è®°'],
                    "æˆªç¨¿æ—¶é—´": row['æˆªç¨¿æ—¶é—´'],
                    "å‰©ä½™å¤©æ•°": calculate_days_left(row['æˆªç¨¿æ—¶é—´']),
                    "åŒ¹é…åˆ†æ": f"è®ºæ–‡æ–¹å‘ä¸ {row['ä¼šè®®ä¸»é¢˜æ–¹å‘']} æœ‰å…³"
                })

        if matching_conferences:
            st.subheader("æ¨èåŒ¹é…çš„ä¼šè®®ï¼š")
            for conf in matching_conferences:
                st.markdown(f"**ä¼šè®®æ¨èï¼š{conf['ä¼šè®®ç³»åˆ—åä¸ä¼šè®®å']}**")
                st.markdown(f"- å®˜ç½‘é“¾æ¥: [{conf['å®˜ç½‘é“¾æ¥']}]({conf['å®˜ç½‘é“¾æ¥']})")
                st.markdown(f"- åŠ¨æ€å‡ºç‰ˆæ ‡è®°: {conf['åŠ¨æ€å‡ºç‰ˆæ ‡è®°']}")
                st.markdown(f"- æˆªç¨¿æ—¶é—´: {conf['æˆªç¨¿æ—¶é—´']} (å‰©ä½™ {conf['å‰©ä½™å¤©æ•°']} å¤©)")
                st.markdown(f"- åŒ¹é…åˆ†æ: {conf['åŒ¹é…åˆ†æ']}")
        else:
            st.info("æœªåŒ¹é…åˆ°åˆé€‚çš„ä¼šè®®ï¼Œè¯·æ ¹æ®å­¦ç§‘æ–¹å‘æŸ¥æ‰¾å…¶ä»–ä¼šè®®")

    except Exception as e:
        st.error(f"ä¼šè®®æ–‡ä»¶è¯»å–å¤±è´¥: {e}")

# ------------------- ä¸»å‡½æ•° -------------------

def main():
    st.set_page_config(layout="wide")
    st.title("ğŸ“„ è®ºæ–‡ä¸ä¼šè®®åŒ¹é…ç³»ç»Ÿ")

    col1, col2 = st.columns(2)

    # å·¦ä¾§ä¸Šä¼ ä¼šè®®æ–‡ä»¶
    with col1:
        st.header("ğŸ“ ä¸Šä¼ ä¼šè®®æ–‡ä»¶")
        conference_file = upload_conference_file()

    # å³ä¾§ä¸Šä¼ è®ºæ–‡æ–‡ä»¶å¹¶åˆ†æ
    with col2:
        st.header("ğŸ“„ ä¸Šä¼ è®ºæ–‡æ–‡ä»¶")
        paper_file = upload_paper_file()

        if paper_file:
            st.success("è®ºæ–‡æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨è§£æ...")

            if paper_file.name.endswith(".pdf"):
                text = extract_text_from_pdf(paper_file)
            elif paper_file.name.endswith(".docx"):
                text = extract_text_from_docx(paper_file)
            else:
                st.error("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")
                return

            title = extract_title(text)
            keywords = extract_keywords(text)

            st.subheader("ğŸ¯ æå–ç»“æœï¼š")

            st.markdown(f"**é¢˜ç›®ï¼ˆä¸­æ–‡ï¼‰ï¼š** {title}")
            st.markdown(f"**Title (English):** {title}")
            st.markdown(f"**å…³é”®è¯ï¼ˆä¸­æ–‡ï¼‰ï¼š** {keywords}")
            st.markdown(f"**Keywords (English):** {keywords}")

            subjects = analyze_paper_subject(text)

            st.subheader("ğŸ“š è®ºæ–‡å­¦ç§‘æ–¹å‘åˆ†æ")
            for subject, weight in subjects.items():
                st.write(f"{subject}: {weight}%")

            perform_matching(conference_file, subjects)
        else:
            st.info("è¯·ä¸Šä¼ è®ºæ–‡æ–‡ä»¶è¿›è¡Œåˆ†æ")

if __name__ == "__main__":
    main()
