import streamlit as st
import pandas as pd
import datetime
from PyPDF2 import PdfReader
from sklearn.feature_extraction.text import TfidfVectorizer
import requests

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(page_title="è®ºæ–‡ä¼šè®®åŒ¹é…æ¨è", layout="wide")
st.title("ğŸ“š è®ºæ–‡ä¼šè®®åŒ¹é…æ¨èç³»ç»Ÿ")

# æ–‡ä»¶ä¸Šä¼ 
st.sidebar.header("1ï¸âƒ£ ä¸Šä¼ è®ºæ–‡ PDF")
pdf_file = st.sidebar.file_uploader("ä¸Šä¼ è®ºæ–‡ PDF æ–‡ä»¶", type=["pdf"])

st.sidebar.header("2ï¸âƒ£ ä¸Šä¼ ä¼šè®®åˆ—è¡¨ Excel")
conf_file = st.sidebar.file_uploader("ä¸Šä¼ ä¼šè®® Excel æ–‡ä»¶", type=["xlsx"])

# è®¾ç½®åŒ¹é…å‚æ•°
st.sidebar.header("3ï¸âƒ£ è®¾ç½®åŒ¹é…å‚æ•°")
days_today = datetime.datetime.now()

# æ–‡ä»¶ä¸Šä¼ åå¤„ç†
if pdf_file and conf_file:
    with st.spinner('æ­£åœ¨å¤„ç†æ–‡ä»¶ï¼Œè¯·ç¨å€™...'):
        try:
            # è¯»å– PDF æ–‡ä»¶
            pdf_reader = PdfReader(pdf_file)
            text = ""
            for page in pdf_reader.pages[:2]:  # æå–å‰ä¸¤é¡µ
                text += page.extract_text() or ""
            
            # æå–è®ºæ–‡å…³é”®è¯
            vectorizer = TfidfVectorizer(stop_words='english', max_features=10)
            tfidf = vectorizer.fit_transform([text])
            paper_keywords = vectorizer.get_feature_names_out()

            st.markdown("### ğŸ“„ è®ºæ–‡æå–ä¿¡æ¯")
            st.write("**è‡ªåŠ¨è¯†åˆ«å…³é”®è¯ï¼š**", ", ".join(paper_keywords))

            # è¯»å–ä¼šè®®æ•°æ®
            df = pd.read_excel(conf_file)

            # ç¡®è®¤ä¼šè®®æ•°æ®æ˜¯å¦åŒ…å« "Keywords" åˆ—
            if 'Keywords' not in df.columns:
                st.error("ä¼šè®®æ•°æ®ä¸­ç¼ºå°‘ 'Keywords' åˆ—ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼ã€‚")
            else:
                conf_keywords = df['Keywords']  # è·å–ä¼šè®®çš„å…³é”®è¯åˆ—
                st.write("**ä¼šè®®å…³é”®è¯ï¼š**", ", ".join(conf_keywords.head()))

                # è®¡ç®—è®ºæ–‡å’Œä¼šè®®ä¹‹é—´çš„ç›¸ä¼¼åº¦
                vectorizer_conf = TfidfVectorizer(stop_words='english')
                tfidf_conf = vectorizer_conf.fit_transform(conf_keywords.astype(str))
                paper_tfidf = vectorizer_conf.transform([text])
                similarity_scores = cosine_similarity(paper_tfidf, tfidf_conf)

                # æ˜¾ç¤ºç›¸ä¼¼åº¦æ’åºçš„ä¼šè®®
                st.markdown("### ğŸ” åŒ¹é…ç»“æœ")
                similarity_df = pd.DataFrame(similarity_scores.T, columns=["ç›¸ä¼¼åº¦"], index=df["Conference Name"])
                similarity_df = similarity_df.sort_values(by="ç›¸ä¼¼åº¦", ascending=False)

                st.write(similarity_df)

                st.success('æ–‡ä»¶å¤„ç†å®Œæˆï¼')

        except Exception as e:
            st.error(f"æ–‡ä»¶å¤„ç†æ—¶å‡ºé”™: {e}")

# ä¼šè®®çˆ¬è™«ç¤ºä¾‹ï¼ˆå¯é€‰ï¼‰
st.sidebar.header("4ï¸âƒ£ å¯é€‰ - ä¼šè®®çˆ¬è™«")
meeting_url = st.sidebar.text_input("è¾“å…¥ä¼šè®®ç½‘ç«™URLï¼ˆå¯é€‰ï¼‰")

if meeting_url:
    try:
        response = requests.get(meeting_url, timeout=10)  # è®¾ç½®è¯·æ±‚è¶…æ—¶ä¸º10ç§’
        response.raise_for_status()  # å¦‚æœçŠ¶æ€ç ä¸ä¸º 200ï¼Œä¼šæŠ›å‡ºå¼‚å¸¸
        st.write("æˆåŠŸçˆ¬å–ç½‘ç«™æ•°æ®")
    except requests.exceptions.RequestException as e:
        st.error(f"çˆ¬è™«è¯·æ±‚å¤±è´¥: {e}")
