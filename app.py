import streamlit as st
import pdfplumber
import docx
import pandas as pd
import datetime
from sentence_transformers import SentenceTransformer, util
import os

# åˆå§‹åŒ–æ¨¡å‹
model = SentenceTransformer('all-MiniLM-L6-v2')

st.title("ğŸ“„ æ™ºèƒ½è®ºæ–‡åŒ¹é…æ¨èä¼šè®®å·¥å…·")

# Session state å­˜å‚¨ä¸Šä¼ æ–‡ä»¶
if 'conference_file' not in st.session_state:
    st.session_state.conference_file = None
if 'paper_file' not in st.session_state:
    st.session_state.paper_file = None

# ä¸Šä¼ ä¼šè®®æ–‡ä»¶ï¼ˆExcelï¼‰
st.header("â‘  ä¸Šä¼ ä¼šè®®æ–‡ä»¶ï¼ˆExcelï¼‰")
conference_uploaded = st.file_uploader("åŒ…å«å­—æ®µï¼šä¼šè®®åç§°ã€ä¼šè®®æ–¹å‘ã€ä¼šè®®ä¸»é¢˜æ–¹å‘ã€ç»†åˆ†å…³é”®è¯ã€åŠ¨æ€å‡ºç‰ˆæ ‡è®°ã€æˆªç¨¿æ—¶é—´ã€å®˜ç½‘é“¾æ¥", type=["xlsx"], key="conf_upload")

if st.button("æ¸…é™¤ä¼šè®®æ–‡ä»¶"):
    st.session_state.conference_file = None
    conference_uploaded = None

if conference_uploaded:
    try:
        conf_df = pd.read_excel(conference_uploaded, engine='openpyxl')
        required_cols = ['ä¼šè®®åç§°', 'ä¼šè®®æ–¹å‘', 'ä¼šè®®ä¸»é¢˜æ–¹å‘', 'ç»†åˆ†å…³é”®è¯', 'åŠ¨æ€å‡ºç‰ˆæ ‡è®°', 'æˆªç¨¿æ—¶é—´', 'å®˜ç½‘é“¾æ¥']
        for col in required_cols:
            if col not in conf_df.columns:
                st.error(f"âŒ ç¼ºå°‘å¿…è¦å­—æ®µï¼š{col}")
                st.stop()
        st.success("âœ… ä¼šè®®æ–‡ä»¶ä¸Šä¼ æˆåŠŸ")
        st.session_state.conference_file = conf_df
    except Exception as e:
        st.error(f"ä¼šè®®æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{e}")
        st.stop()

# ä¸Šä¼ è®ºæ–‡æ–‡ä»¶ï¼ˆWord æˆ– PDFï¼‰
st.header("â‘¡ ä¸Šä¼ è®ºæ–‡æ–‡ä»¶ï¼ˆWord æˆ– PDFï¼‰")
paper_uploaded = st.file_uploader("ä¸Šä¼ è®ºæ–‡æ–‡ä»¶ï¼ˆPDF æˆ– Wordï¼‰", type=["pdf", "docx"], key="paper_upload")

if st.button("æ¸…é™¤è®ºæ–‡æ–‡ä»¶"):
    st.session_state.paper_file = None
    paper_uploaded = None

if paper_uploaded:
    try:
        def extract_text(file):
            if file.name.endswith(".pdf"):
                with pdfplumber.open(file) as pdf:
                    return "\n".join(page.extract_text() for page in pdf.pages if page.extract_text())
            elif file.name.endswith(".docx"):
                doc = docx.Document(file)
                return "\n".join(p.text for p in doc.paragraphs)
            else:
                return ""

        paper_text = extract_text(paper_uploaded)

        # ç®€å•åˆ†æ®µæå–æ‘˜è¦ã€å…³é”®è¯ã€ç»“è®º
        def extract_sections(text):
            text_lower = text.lower()
            abstract, keywords, conclusion = "", "", ""

            if "abstract" in text_lower:
                abstract = text[text_lower.find("abstract"):text_lower.find("introduction") if "introduction" in text_lower else 1000]
            if "keywords" in text_lower:
                keywords = text[text_lower.find("keywords"):text_lower.find("\n", text_lower.find("keywords") + 8)]
            if "conclusion" in text_lower:
                conclusion = text[text_lower.find("conclusion"):text_lower.find("references") if "references" in text_lower else len(text)]

            return abstract + " " + keywords + " " + conclusion

        extracted_text = extract_sections(paper_text)
        paper_embedding = model.encode(extracted_text, convert_to_tensor=True)

        results = []

        for _, row in st.session_state.conference_file.iterrows():
            all_text = f"{row['ä¼šè®®æ–¹å‘']} {row['ä¼šè®®ä¸»é¢˜æ–¹å‘']} {row['ç»†åˆ†å…³é”®è¯']}"
            conf_embedding = model.encode(all_text, convert_to_tensor=True)
            score = float(util.cos_sim(paper_embedding, conf_embedding)[0])

            # åŒ¹é…å…³é”®è¯è¯¦ç»†å±•ç¤º
            matched_keywords = []
            if isinstance(row['ç»†åˆ†å…³é”®è¯'], str):
                for keyword in row['ç»†åˆ†å…³é”®è¯'].split(','):
                    if keyword.strip().lower() in extracted_text.lower():
                        matched_keywords.append(keyword.strip())

            try:
                deadline = pd.to_datetime(row['æˆªç¨¿æ—¶é—´'], errors='coerce')
                days_left = (deadline - datetime.datetime.now()).days if not pd.isnull(deadline) else "æœªçŸ¥"
            except:
                days_left = "æœªçŸ¥"

            results.append({
                "ä¼šè®®åç§°": row['ä¼šè®®åç§°'],
                "å®˜ç½‘é“¾æ¥": row['å®˜ç½‘é“¾æ¥'],
                "åŒ¹é…åº¦": score,
                "åŒ¹é…å…³é”®è¯": matched_keywords,
                "è·ç¦»æˆªç¨¿æ—¶é—´": days_left
            })

        top_results = sorted(results, key=lambda x: x["åŒ¹é…åº¦"], reverse=True)[:2]

        st.header("ğŸ¯ æ¨èä¼šè®®ç»“æœ")
        for res in top_results:
            st.subheader(res["ä¼šè®®åç§°"])
            st.markdown(f"ğŸ”— [ä¼šè®®å®˜ç½‘é“¾æ¥]({res['å®˜ç½‘é“¾æ¥']})")
            st.markdown(f"ğŸ“Œ åŒ¹é…ç†ç”±ï¼š**ç›¸ä¼¼åº¦ {res['åŒ¹é…åº¦']:.2f}**ï¼Œå…³é”®è¯åŒ¹é…ï¼š{', '.join(res['åŒ¹é…å…³é”®è¯']) if res['åŒ¹é…å…³é”®è¯'] else 'æ— å…³é”®è¯åŒ¹é…'}")
            st.markdown(f"â° è·ç¦»æˆªç¨¿æ—¶é—´ï¼š{res['è·ç¦»æˆªç¨¿æ—¶é—´']} å¤©")
            st.markdown("---")

    except Exception as e:
        st.error(f"å¤„ç†æ—¶å‡ºé”™ï¼š{e}")
