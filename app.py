import streamlit as st
import pandas as pd
import re
from docx import Document
import fitz  # PyMuPDF
from googletrans import Translator

# --- åˆå§‹åŒ–ç¿»è¯‘å™¨ ---
translator = Translator()

# --- PDFè§£æå¢å¼ºç‰ˆ ---
def extract_text_from_pdf(file):
    try:
        doc = fitz.open(stream=file.read(), filetype="pdf")
        
        # ä¼˜å…ˆå°è¯•è¯»å–ç›®å½•ä¸­çš„æ ‡é¢˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        toc_title = ""
        if doc.get_toc():
            toc_title = doc.get_toc()[0][1]  # å–ç¬¬ä¸€çº§ç›®å½•
        
        # æ­£æ–‡è§£æç­–ç•¥ï¼šç»“åˆæ–‡æœ¬å’Œå­—ä½“ä¿¡æ¯
        title_candidates = {}
        
        text = ""
        for page in doc:
            blocks = page.get_text("dict")["blocks"]
            for block in blocks:
                if block["type"] == 0:  # Only text blocks
                    for line in block["lines"]:
                        current_text = "".join([span["text"] for span in line["spans"]])
                        font_size = line["spans"][0]["size"] if line["spans"] else 12

                        # Rule: Large font (likely title)                        
                        if font_size > 20 and len(current_text.strip()) > 8:
                            title_candidates[current_text.strip()] = font_size

                        text += current_text + "\n"
        
        # Return the largest font text as title candidate                
        if title_candidates:
            sorted_titles = sorted(title_candidates.items(), key=lambda x: x[1], reverse=True)            
            toc_title = sorted_titles[0][0]

        return toc_title, text.strip()
    
    except Exception as e:
        st.error(f"PDFè§£æå¤±è´¥: {str(e)}")
        return "", ""

# --- Wordè§£æå¢å¼ºç‰ˆ --- 
def extract_text_from_word(file):
    try:
        doc = Document(file)
        
         # Check for built-in title style (MS Word specific)
         title_candidate = ""
         for para in doc.paragraphs:
             if para.style.name.lower() == 'title':
                 title_candidate = para.text.strip()
                 break
        
         full_text = "\n".join([para.text for para in doc.paragraphs])
         return title_candidate, full_text
    
     except Exception as e:
         st.error(f"Wordè§£æå¤±è´¥: {str(e)}") 
         return "", "" 

# --- AIå¢å¼ºçš„æ ‡é¢˜è¯†åˆ« --- 
def extract_title(text, filename=""):    
     """æ™ºèƒ½è¯†åˆ«æ ‡é¢˜çš„ä¸‰å±‚ç­–ç•¥"""
     
     # Rule1: PDF/Wordå†…ç½®æ ‡é¢˜ä¿¡æ¯ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
     if hasattr(text, 'title') and text.title:  
         return text.title

     # Rule2: Abstractå‰çš„ç¬¬ä¸€æ®µéå…ƒæ•°æ®æ–‡æœ¬  
     abstract_positions = [
         m.start() for m in re.finditer(
             r'(?i)(\babstract\b|æ‘˜è¦|[\n\r]\s*abstract\s*[\n\r])', 
             str(text))
     ]
     
     if abstract_positions:
         pre_abstract = str(text)[:abstract_positions[0]]
         candidates = [
             line.strip() for line in pre_abstract.split('\n') 
             if len(line.strip()) > 10 and not is_metadata(line)
         ]
         if candidates:
             return candidates[-1]

     # Rule3: Heuristic rules + ML fallback (ç®€åŒ–ä¸ºè§„åˆ™)  
     lines = [line.strip() for line in str(text).split('\n') if line.strip()]
     
     metadata_keywords = [
         'author', 'received', 'accepted', 'doi', 'arxiv', '@', 'university',
         'email', 'proceedings', 'volume', 'issue', 'pp.'
     ]
     
     for line in lines:
         conditions = [
             len(line) > len(filename.replace('.pdf','').replace('.docx','')),
             not any(kw in line.lower() for kw in metadata_keywords),
             sum(c.isupper() for c in line)/len(line) <0.3,
             not line.startswith(('Â©','Copyright','http')),
             not re.match(r'^\W*\d{4}\W*$', line) # Exclude pure year like [2023]
         ]
         
         if all(conditions):
             return re.sub(r'[\r\n\t]+', ' ', line).strip()

     return filename.split('.')[0] + " (è‡ªåŠ¨è¯†åˆ«)"

def is_metadata(line):
     """åˆ¤æ–­æ˜¯å¦ä¸ºä½œè€…/æœºæ„ç­‰å…ƒæ•°æ®è¡Œ"""
     patterns = [
         r'^\s*\S+@\S+\.\S+\s*$',          # Emailæ ¼å¼  
         r'^(\w\.\s+)*\w+(\s+et al\.)?$',   # Author names  
         r'^.*univ\w*,\s*\w+.*$'           # University info  
     ]
     
     return any(re.search(p, line, re.I) for p in patterns)

# --- AIå¢å¼ºçš„å…³é”®è¯æå– ---  
def extract_keywords(text):    
     """å››å±‚å…³é”®è¯æŠ½å–ç­–ç•¥"""
     
     # Rule1: Explicit keyword sections (å¤šè¯­è¨€æ”¯æŒ)  
     keyword_labels_en = [
         r'keywords?\s*[:;\-\â€”]', 
         r'index terms\s*[:;\-\â€”]',
         r'key\s*words?\s*[:;\-\â€”]'
     ]
     
     keyword_labels_zh = [
         r'å…³é”®[è¯å­—]\s*[:;\-\â€”]',
         r'ç´¢å¼•æœ¯è¯­\s*[:;\-\â€”]'
     ]
     
     all_labels = keyword_labels_en + keyword_labels_zh
    
     for pattern in all_labels:
         match = re.search(
             pattern + r'\s*(.*?)(?:\n|$)', 
             str(text), re.I | re.DOTALL)
         
         if match and len(match.group(1).strip()) >3:
             keywords_str = clean_keywords(match.group(1))
             if keywords_str: 
                 return keywords_str

     # Rule2: Bullet-point lists near abstract  
     bullet_section = re.search(
         r'(?:abstract|æ‘˜è¦).*?([â€¢â–ªâ– â€£â¦¿â¢â£â¤â¦¾]\s*.+?)(?:\n{2}|\Z)', 
         str(text), re.I | re.DOTALL)  
     
     if bullet_section:  
         keywords_str = clean_keywords(bullet_section.group(1))
         if keywords_str: 
             return keywords_str

     # Rule3: High-frequency nouns (ç®€åŒ–ä¸ºè§„åˆ™å®ç°)  
     nouns_found = re.findall(
         r'\b[A-Z][a-z]{3,}\b(?![\.\d])', str(text))  
     
     if nouns_found:  
         from collections import Counter  
         top_nouns = [w for w,cnt in Counter(nouns_found).most_common(5)]
         return "; ".join(top_nouns)

     # Rule4: Fallback to manual selection UI  
     st.warning("âš ï¸æœªèƒ½è‡ªåŠ¨è¯†åˆ«å…³é”®è¯")  
     manual_keywords = st.text_input(
         "è¯·æ‰‹åŠ¨è¾“å…¥å…³é”®è¯ï¼ˆç”¨åˆ†å·åˆ†éš”ï¼‰:", 
         key="manual_kws")  
     
     return manual_keywords or "æ— å…³é”®è¯"

def clean_keywords(raw_str):    
     """æ¸…æ´—å…³é”®è¯å­—ç¬¦ä¸²"""    
     cleaned_str=re.sub(
         r'[\r\n\t]+', ' ', raw_str).strip(';,. ')  
     
      # Split by common separators  
      separators_regex=r'[;,\â€¢\â–ª\â– â€£â¦¿â¢â£â¤â¦¾]\s*|\band\b|\bä¸\b|\bor\b|\bæˆ–\b'
      split_kws=filter(None, [
          kw.strip().capitalize() 
          for kw in re.split(separators_regex, cleaned_str) 
          if len(kw.strip())>2 and not kw.isdigit()
      ])  
      
      return "; ".join(sorted(set(split_kws), key=len, reverse=True))

# --- Streamlit UI ---  
def main():    
      st.set_page_config(layout="wide", page_title="å­¦æœ¯è®ºæ–‡æ™ºèƒ½è§£æ")  

      with st.sidebar.expander("âš™ï¸é«˜çº§è®¾ç½®"):
          trans_lang=st.selectbox(
              "ç¿»è¯‘ç›®æ ‡è¯­è¨€:", ["zh","en","ja","fr"],
              index=0)  

      uploaded_file=st.file_uploader(
          "ğŸ“¤ä¸Šä¼ å­¦æœ¯è®ºæ–‡(PDF/Word)", 
          type=["pdf","docx"])  

      if uploaded_file:        
          with st.spinner(f"æ­£åœ¨è§£æ {uploaded_file.name}..."):            
              file_ext=uploaded_file.name.split('.')[-1].lower()  

              try:                
                  if file_ext=="pdf":
                      detected_title, full_text=extract_text_from_pdf(uploaded_file)                
                  else:                    
                      detected_title, full_text=extract_text_from_word(uploaded_file)  

                  final_title=extract_title(
                      full_text or detected_title,
                      filename=uploaded_file.name)  

                  keywords=extract_keywords(full_text)  

                  trans_title=translator.translate(
                      final_title, dest=trans_lang).text  

                  trans_kws=translator.translate(
                      keywords, dest=trans_lang).text  

              except Exception as e:                
                  st.error(f"è§£æé”™è¯¯:{str(e)}")                
                  st.stop()  

          # ---ç»“æœæ˜¾ç¤º---          
          col1,col2=st.columns([1,2])  

          with col1:            
              st.subheader("ğŸ”å…ƒæ•°æ®æå–")            
              with st.expander("ğŸ“æ ‡é¢˜åˆ†æ", expanded=True):                
                  st.write(f"**åŸå§‹æ ‡é¢˜**:\n\n`{final_title}`")                
                  st.write(f"**ç¿»è¯‘ç»“æœ**:\n\n`{trans_title}`")  

              with st.expander("ğŸ”‘å…³é”®è¯åˆ†æ"):                
                  st.write(f"**åŸå§‹å…³é”®è¯**:\n\n`{keywords}`")                
                  st.write(f"**ç¿»è¯‘ç»“æœ**:\n\n`{trans_kws}`")  

          with col2:            
              tab1,tab2=st.tabs(["ğŸ“œå…¨æ–‡é¢„è§ˆ","ğŸ“Šæ™ºèƒ½åˆ†æ"])  

              with tab1:                
                  st.text_area(
                      label="å…¨æ–‡å†…å®¹ï¼ˆå‰1000å­—ç¬¦ï¼‰:",                      
                      value=full_text[:1000]+"...",
                      height=300)  

              with tab2:                
                  analyze_button=st.button(
                      "âœ¨æ‰§è¡Œæ·±åº¦åˆ†æ",                    
                      help="ä½¿ç”¨AIæ¨¡å‹åˆ†æç ”ç©¶æ–¹æ³•å’Œåˆ›æ–°ç‚¹")  

if __name__=="__main__":    
      main()
